---
layout: single
title:  "Efficient networking in Kubernetes with service type Load Balancer"
categories: [kubernetes]
tags: [kubernetes]
excerpt: "Load balancing directly to pods and bypassing kube-proxy in public cloud environments" #this is a custom variable meant for a short description to be displayed on home page
---
### Summary
When pods are directly routable from outside of a Kubernetes cluster, load balancing connections into the cluster can target the pods directly, instead of NodePort values. Targeting pods directly allows for better networking performance and visibility into pod health from outside of the cluster.

### My Kubernetes application
I have an application that I will to deploy to Kubernetes. It has two tiers. The 'frontend' tier is a web server that serves HTTP responses to requests from browsers. The 'backend' tier is an API that is stateful, containing a list of silly job titles. We can add more to this list if we please. The 'frontend' web tier calls the 'backend' tier in order to display a silly job title to a user, along with some information about the user's request.

Because my frontend and backend tier are separate services, I can manage them independently. In this example, we will scale my stateless frontend tier only. I cannot easily scale my backend tier, which is stateful. In future, if I want to make changes to my backend tier - for example to migrate to using a database - a development team could do so in a new version independently of any development team working on the frontend tier.

### Let's deploy and expose this application in Kubernetes

#### Deploying the backend pods

`kubectl apply -f backend-deployment.yaml`

This is what `backend-deployment.yaml` looks like. Notice I will include the line `replicas: 1` to indicate I would like to deploy a single pod only.

````yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: backend
  namespace: jobs
  name: backend-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app: backend
  template:
    metadata:
      labels:
        app: backend
    spec:
      containers:
      - image: michaelwoleary/jobs-api
        name: backend
````

Great! We now have a deployment of a single pod running our backend API function. Let's verify this:

`kubectl get deployment -n jobs`

````bash
NAME                 READY   UP-TO-DATE   AVAILABLE   AGE
backend-deployment   1/1     1            1           55s
````

##### Further inspection of our backend deployment

The verification above tells me information about my deployment, but it doesn't give me information about the individual pod, like IP address or which node it was assigned to. 

Let's learn the IP address of my pod by running `kubectl get pods`. We'll add `-l app-backend` to get only pods matching our deployment, and `-o wide` so that the output includes the pod IP address.

`kubectl get pod -n jobs -l app=backend -o wide`

````yaml
NAME                                 READY   STATUS    RESTARTS   AGE   IP             NODE                                NOMINATED NODE   READINESS GATES
backend-deployment-f7f668745-5f5kz   1/1     Running   0          23m   10.224.0.102   aks-agentpool-12467481-vmss000000   <none>           <none>
````

Great! Now I know my pod IP address is 10.244.0.102. But how will my frontend pods reach this IP address when they need to call the backend API function?

#### Exposing the backend pods within the cluster

A Service in Kubernetes allows pods to be easily discoverable and reachable across the pod network. We know that our frontend pods will need to discover and reach a backend pod. To make this discovery possible, we expose our backend pod with a Service.

`kubectl apply -f backend-service.yaml`

This is what `backend-service.yaml` looks like:

````yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    app: backend
  name: backend-service
  namespace: jobs
spec:
  ports:
  - name: backend
    port: 3000
    protocol: TCP
    targetPort: 3000
  selector:
    app: backend
  type: ClusterIP
````

Let's verify the creation of this service:

`kubectl get service -n jobs`

````bash
NAME              TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE
backend-service   ClusterIP   10.0.188.165   <none>        3000/TCP   3s
````

We can see our service exists with it's own ClusterIP address of 10.0.188.165 and that we have a single port of 3000/TCP at which our service is reachable. But how is this IP address discovered?

##### DNS and our backend service

Our service IP address must still be discovered by any pod before it can initiate network connections to this IP address. This discovery happens using DNS.

When a Service is created in Kubernetes, a DNS record is created with the fqdn of `<service-name>.<namespace>.svc.cluster.local` 

Let's verify this by performing a DNS lookup of this service name from another pod. The below command will create a pod with a container running a dns utility tool. In the same command we will pass `sh -c "nslookup backend-service"` to instruct that container to perform a DNS lookup of our service name. The `--rm` will ensure our pod is deleted after the command is run.

`kubectl run -n jobs -i dnsutils --image=gcr.io/kubernetes-e2e-test-images/dnsutils:1.3 --rm -- sh -c "nslookup backend-service"`

````bash
If you don't see a command prompt, try pressing enter.
Server:         10.0.0.10
Address:        10.0.0.10#53

Name:   backend-service.jobs.svc.cluster.local
Address: 10.0.188.165
````

We have verified that our service can be resolved by its name (backend-service) to its IP address (10.0.188.165). Another pod in Kubernetes may now target this service using its name. Connections made to the service IP address and port will be translated to a new destination IP address and port of a pod that is exposed by this service.

##### How does our Service IP address get translated to our Pod IP address?

As we saw above, our service has a ClusterIP value of 10.0.188.165 that a client can target, but our backend pod has an IP address of 10.244.0.102. How does this IP address translation occur?

When a Service is created, the object is stored in etcd, the database that stores the entire configuration of a cluster. Many components of Kubernetes will then interact with kube-apiserver to query or update related information. 

In the case of network connections between pods, kube-proxy is the mechanism by which network connections from pods are translated from a service IP address and port to a pod IP address and port. Kube-proxy does this by querying kube-apiserver, learning the IP addresses and ports of pods belonging to a service, and manipulating iptables rules in order to perform the necessary address/port translation and load-balancing.

The objects stored in etcd that kube-proxy queries in order to achieve this are Endpoints.

##### Endpoints and Services

For every Service created, an Endpoints object will automatically be created in etcd. This Endpoints object will contain the IP addresses and ports of all healthy pods that match a given Service. 

Let's check this out for ourselves: `kubectl get endpoints -n jobs`

````bash
NAME              ENDPOINTS           AGE
backend-service   10.224.0.102:3000   30
````

The above output shows that an Endpoints object was created with the name of `backend-service`, and that there is a single endpoint in this list: 10.224.0.102:3000. This is the IP address and port of our backend pod.

Kube-proxy watches the kube-apiserver for changes to Services and Endpoints.

In this way, kube-proxy is able to immediately update iptables rules so that connections made from a pod to our service IP address (10.0.188.165) will be translated to the destination IP of our pod (10.224.0.102), and that traffic in the return flow is appropriately translated also.

#### Verifying our backend service is running

Before moving on to our front-end service, let's verify our backend service is properly deployed. The below command will create a pod with a container running the curl utlity. We will issue a command of `sh -c "curl backend-service:3000"` to instruct the container to make a web request to our service on port 3000. The `--rm` will ensure our pod is deleted after the command is run.

`kubectl run -n jobs -i curl --image=curlimages/curl --rm -- sh -c "curl backend-service:3000"`

````bash
100    37  100    37    0     0     14      0  0:00:02  0:00:02 --:--:--    14{"job":"Head of Getting Things Done"}
pod "curl" deleted
````

Notice that the response to the curl command is JSON-formatted: `{"job":"Head of Getting Things Done"}`. This is verification that our backend pod is running and reachable via our service from another pod using DNS.

#### Deploying and exposing the frontend pods

`kubectl apply -f frontend-deployment.yaml`

This is what `frontend-deployment.yaml` looks like. This time I will specify `replicas: 3` to indicate I would like three pods deployed.

````yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: frontend
  namespace: jobs
  name: frontend-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: frontend
  template:
    metadata:
      labels:
        app: frontend
    spec:
      containers:
      - image: michaelwoleary/jobs-app
        name: frontend

````

We now have a deployment of three pods running our front end web service. Let's verify this:

`kubectl get deployment -n jobs frontend-deployment`

````bash
NAME                 READY   UP-TO-DATE   AVAILABLE   AGE
frontend-deployment  3/3     3            3           12s
````

#### Inspecting our frontend deployment

Similar to our backend deployment of a single pod, let's check the details of our frontend deployment. 

`kubectl get pod -n jobs -l app=frontend -o wide`

````bash
NAME                                   READY   STATUS    RESTARTS   AGE   IP             NODE                                NOMINATED NODE   READINESS GATES
frontend-deployment-6ffcfdb55d-529tw   1/1     Running   0          11m   10.224.0.107   aks-agentpool-12467481-vmss000000   <none>           <none>
frontend-deployment-6ffcfdb55d-dxvb8   1/1     Running   0          11m   10.224.0.16    aks-agentpool-12467481-vmss000000   <none>           <none>
frontend-deployment-6ffcfdb55d-q2q89   1/1     Running   0          11m   10.224.0.109   aks-agentpool-12467481-vmss000000   <none>           <none>
````

Similar to our backend service, we see that pods are running and have unique IP addresses. Unlike our backend service, our frontend service does not need to be reachable from other pods in the cluster. It must be reached by users with web browswers outside of the cluster.

#### Exposing the frontend pods 

To expose our frontend pods we will use a service of type LoadBalancer. When a service of this type is created, a component within Kubernetes called the cloud controller manager will notice the creation of this service and automatically create a load balancer in the cloud environment.

`kubectl apply -f frontend-service.yaml`

````yaml
apiVersion: v1
kind: Service
metadata:
  labels:
    app: frontend
  name: frontend-service
  namespace: jobs
spec:
  ports:
  - name: frontend
    port: 80
    protocol: TCP
    targetPort: 3000
  selector:
    app: frontend
  type: LoadBalancer
  ````

Now let's verify the creation of this service.

`kubectl get service -n jobs frontend-service`

````bash
NAME               TYPE           CLUSTER-IP    EXTERNAL-IP   PORT(S)        AGE
frontend-service   LoadBalancer   10.0.216.86   52.254.30.5   80:31803/TCP   2m16s
````

Let's also verify the creation of an Endpoints resource:

`kubectl get endpoints -n jobs frontend-service`

We can see from the above output that our service is reachable via an Azure load balancer, with an IP address of 52.254.30.5 and port 80. If we make a browser request to this IP address, we can see that the load balancer will receive our request, and then perform destination IP and port translation. The IP address will belong to a node behind the load balancer, and the port will be 31803. After our request reaches the node, kube-proxy will again translate the IP and port to that of a pod within the service. 

Great! Let's now check out our application that has been exposed to the public Internet.

